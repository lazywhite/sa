最后，elasticsearch团队提供了对运行大集群的几点优化建议：

1.设置ES_HEAP_SIZE环境变量，保证JVM使用的最大和最小内存用量相同。如果设置的最小和最大内存不一样，这意味着当jvm需要额外的内存时（最多达到最大内存的大小），它会阻塞java进程来分配内存给它。结合使用旧版本的java情况就可以解释为什么集群中的节点会停顿、出现高负载和不断的进行内存分配的情况。elasticsearch团队建议给es设置50%的系统内存
2.缩短recover_after_time超时配置，这样恢复可以马上进行，而不是还要再等一段时间。
3.配置minimum_master_nodes，避免多个节点长期暂停时，有些节点的子集合试图自行组织集群，从而导致整个集群不稳定。
4.在es初始恢复的时候，一些节点用完了磁盘空间。这个不知道是怎样发生的，因为整个集群只使用了总空间的67%，不过相信这是由于之前的高负载和旧java版本引起的。elasticsearch的团队也在跟进这个问题。

-----------------------
ES node type
1. data node
# only store index
http.enabled=false(they will use transport module to communicate with each other)
node.master=false
node.data=true
2. dedicated master node (at least 3 node)
# do not process any request just ensure the instability of cluster
node.master=true
node.data=false
3. client node
# only accept index query from client
node.master=false
node.data=false

---------------------
由于集群并未开启allocate， 导致重启数据节点后，全部变为unassigned shards
集群健康状态为黄色，并不会停止录入数据
curl localhost:9200/_cluster/settings

# v0.90.x and earlier
curl -XPUT 'localhost:9200/_settings' -d '{
    "index.routing.allocation.disable_allocation": false
}'

# v1.0+
curl -XPUT 'localhost:9200/_cluster/settings' -d '{
    "transient" : {
        "cluster.routing.allocation.enable" : "all"
    }
}'
---------------------
curl http://localhost:9200/_cat/recovery?v
curl localhost:9200/_cat/shards
curl  'http://localhost:9200/_recovery?pretty&human'
------------------



input {
    file {
            type => "technical"
            path => "/home/technical/log"
    }
    file {
            type => "business"
            path => "/home/business/log"
    }
} 


filter {
    if [type] == "technical" {
            # processing .......
    }
    if [type] == "business" {
            # processing .......
    }
}


=========================

The correct way to restart a cluster is to do a rolling restart using the shutdown API.

This works by:

Disabling shard allocation
Restarting one node (cluster goes yellow)
Wait until it rejoins the cluster
Re-enable shard allocation
Wait until shards are reallocated (cluster goes green)
Repeat on other nodes.

=======================
web client
    Sense
        ./bin/kibana plugin --install elastic/sense
monitoring
    Marvel
        bin/plugin install license
        bin/plugin install marvel-agent
        bin/kibana plugin --install elasticsearch/marvel/latest

security
    Shield
alert
    Watcher
es as service
    Cloud
virtualize 
    Kibana
collect
    Logstash
collec, parse, ship
    Beats
es for hadoop
rivers
curator



cluster
    node
        index
            type
                document (json)
    


shard
    horizontally split/scale your content volume
    distribute and parallelize operations across shards

replication
    high availability
    scale out search volumn/throughout since search can be executed on all replicas 

replica number can be changed dynamiclly but shard number can't 

