Hadoop组件 
	1. Hadoop Common: hadoop基础库
	2. HDFS: 分布式文件系统
	3. Hadoop YARN: mapreduce框架
	4. Hadoop MapReduce: mapreduce实现

Mapreduce
	job tracker(单点master)
	task tracker(最好运行在datanode, 可以有多个)
	
HDFS
	NameNode
    Secondary NameNode
	DataNode
	File
		Block: 默认大小为64Mkko
		Replication: 每个block的备份数ko

NameNode高可用
    1. 启动多个NameNode, 只有一个处于Active状态
    2. 通过相互独立的"journal node"进程进行通信, 同步Active上namespace的更改到本地
    3. DataNode需要同时向多个NameNode会报
    4. 共享edits log, 达到快速failover的目的, 共享的方式有以下两种
        Quorum Journal Manager (QJM)
        NFS
    5. 需要借助zookeeper进行选举和切换
	
Hadoop Streaming
	用任意类型的脚本或语言创建hadoop jobs
    自定义mapper.py, reducer.py
    运行 #hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py


Job执行流程
	1.用户提交job给jobtracker
	2.jobtracker查询namenode, 获得数据的存放位置
	3.jobtracker筛选出离数据近或就在数据节点的taskTracker
	4.jobTracker分配任务给TaskTracker
	5.TaskTracker的执行状态被监控, 必须向jobtracker发送心跳信息, 如果不发送, task被重新调度给其他TaskTracker

HDFS GUI
	http://localhost:50070/

job执行管理
	http://localhost:8088/
	
更改默认块大小
  	hadoop fs -D dfs.blocksize=268435456 -copyFromLocal /hirw-starterkit/hdfs/commands/dwp-payments-april10.csv blksize/dwp-payments-april10_256MB.csv 

Hadoop涉及的端口
    9000    fs.defaultFS，如：hdfs://172.25.40.171:9000
    50070   webgui, webhdfs
    14000   httpfs


WebHDFS 与 HTTPFS
    webhdfs默认开启, httpfs需要额外启动服务httfs.sh
    webhdfs需要client同时访问所有节点, httpfs只需访问一个节点, 但数据只从此节点IO, 因此会有单点故障问题
