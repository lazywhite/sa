Hadoop组件 
	1. Hadoop Common: hadoop基础库
	2. HDFS: 分布式文件系统
	3. Hadoop YARN: mapreduce框架
	4. Hadoop MapReduce: mapreduce实现

HDFS
	NameNode
    Secondary NameNode
	DataNode
	File
		Block: 默认大小为64Mkko
		Replication: 每个block的备份数ko

NameNode高可用
    1. 启动多个NameNode, 只有一个处于Active状态
    2. 通过相互独立的"journal node"进程进行通信, 同步Active上namespace的更改到本地
    3. DataNode需要同时向多个NameNode会报
    4. 共享edits log, 达到快速failover的目的, 共享的方式有以下两种
        Quorum Journal Manager (QJM)
        NFS
    5. 需要借助zookeeper进行选举和切换
	
Hadoop Streaming
	用任意类型的脚本或语言创建hadoop jobs
    自定义mapper.py, reducer.py
    运行 #hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py


HDFS GUI
	http://localhost:50070/

job执行管理
	http://localhost:8088/
	
更改默认块大小
  	hadoop fs -D dfs.blocksize=268435456 -copyFromLocal /hirw-starterkit/hdfs/commands/dwp-payments-april10.csv blksize/dwp-payments-april10_256MB.csv 

Hadoop涉及的端口
    9000    fs.defaultFS，如：hdfs://172.25.40.171:9000
    50070   webgui, webhdfs
    14000   httpfs
    8030,8031,8032    resource_manager


WebHDFS 与 HTTPFS
    webhdfs默认开启, httpfs需要额外启动服务httfs.sh
    webhdfs需要client同时访问所有节点, httpfs只需访问一个节点, 但数据只从此节点IO, 因此会有单点故障问题



yarn1
    jobtracker
    tasktracker

    这种方式还是有一定的弊端的

    tasktracker出现故障，会导致整个任务计算失败。
    jobtracker压力过大，既要负责全局的任务分配，还需要时刻与tasktracker沟通。

yarn2
    resource manager
    application master
    node manager


    流程大致如下

    client客户端向yarn集群(resourcemanager)提交任务
    resourcemanager选择一个node创建appmaster
    appmaster根据任务向rm申请资源
    rm返回资源申请的结果
    appmaster去对应的node上创建任务需要的资源（container形式，包括内存和CPU, 不同于docker container）
    appmaster负责与nodemanager进行沟通，监控任务运行
    最后任务运行成功，汇总结果。

    
    三种调度器
        FIFO
        Capacity
        Fair


执行引擎
    MR
    spark
    tez
