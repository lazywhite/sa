Hadoop组件 
	1. Hadoop Common: hadoop基础库
	2. HDFS: 分布式文件系统
	3. Hadoop YARN: 任务调度和资源管理框架
	4. Hadoop MapReduce: yarn具体实现
    5. ozone(object store)
    6. submarine(machine learning engine)

NFS Gateway
    允许通过NFS协议挂载HDFS

HDFS
	NameNode
    Secondary NameNode(仅配合单namenode使用)
	DataNode
	File
		Block: 默认大小为128MB
		Replication: 每个block的备份数ko

NameNode高可用
    1. 启动多个NameNode, 只有一个处于Active状态
    2. 通过相互独立的journalNode进程进行通信, 同步Active上namespace的更改到本地
    3. DataNode需要同时向多个NameNode汇报
    4. 需要借助zookeeper进行选举和切换
    5. 高可用namenode，secondaryNameNode需关闭

ResourceManager高可用
    1. zookeeper
    2. yarn-site.xml
    3. yarn rmadmin
    
	
Hadoop Streaming
	用任意类型的脚本或语言创建hadoop jobs
    自定义mapper.py, reducer.py
    运行 #hadoop jar contrib/streaming/hadoop-streaming-1.2.1.jar -input myinput -output myoutput -mapper /home/expert/hadoop-1.2.1/mapper.py -reducer /home/expert/hadoop-1.2.1/reducer.py


HDFS GUI
	http://localhost:50070/

job执行管理
	http://localhost:8088/
	
更改默认块大小
  	hadoop fs -D dfs.blocksize=268435456 -copyFromLocal /hirw-starterkit/hdfs/commands/dwp-payments-april10.csv blksize/dwp-payments-april10_256MB.csv 

Hadoop涉及的端口
    9000    fs.defaultFS，如：hdfs://172.25.40.171:9000
    50070   webgui, webhdfs
    14000   httpfs
    8030,8031,8032    resource_manager


WebHDFS 与 HTTPFS
    webhdfs默认开启, httpfs需要额外启动服务httfs.sh
    webhdfs需要client同时访问所有节点, httpfs只需访问一个节点, 但数据只从此节点IO, 因此会有单点故障问题



yarn1
    jobtracker
    tasktracker

    这种方式还是有一定的弊端的

    tasktracker出现故障，会导致整个任务计算失败。
    jobtracker压力过大，既要负责全局的任务分配，还需要时刻与tasktracker沟通。

yarn2
    resource manager
    application master
    node manager


    流程大致如下

    client客户端向yarn集群(resourcemanager)提交任务
    resourcemanager选择一个node创建appmaster
    appmaster根据任务向rm申请资源
    rm返回资源申请的结果
    appmaster去对应的node上创建任务需要的资源（container形式，包括内存和CPU, 不同于docker container）
    appmaster负责与nodemanager进行沟通，监控任务运行
    最后任务运行成功，汇总结果。

    
    三种调度器
        FIFO
        Capacity
        Fair


执行引擎
    MR
    spark
    tez
