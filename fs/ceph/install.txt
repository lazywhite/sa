安装手册
    http://docs.ceph.com/docs/master/start/quick-ceph-deploy/
    http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/


准备工作
    os: centos-7
    ceph: 12.2.6 luminous
    磁盘使用xfs
    1. ntp
    2. ustc.repo, ceph.repo, epel
    3. selinux firewalld
    4. ssh-copy-id
    5. disk without partition
    6. systemctl stop NetworkManager; systemctl disable NetworkManager
    7. systemctl start network; systemctl enable network
    8. /etc/resolv.conf
    9. yum.conf # forward proxy

(deploy): 
    yum -y install ceph-deploy
    mkdir ceph-cluster; cd ceph-cluster
    ceph-deploy --username root new ceph1 # 初始mon节点, 在当前文件夹生成配置文件
    ceph-deploy --username root install --no-adjust-repos ceph1 ceph2 # 安装ceph
    ceph-deploy mon create-initial # 启动第一个mon节点
    # modify ceph.conf
    ceph-deploy admin ceph1 ceph2 # 拷贝集群文件至目标节点, 使其可管理集群
    ceph-deploy mgr create ceph1 # 启动manager

    systemctl enable rbdmap 
    ceph-disk list

    #创建osd, 成功后/var/lib/ceph/osd/ceph-<int>里面有软连接指向lvm设备
    ceph-deploy osd create --data /dev/sdb --fs-type xfs ceph1
    ceph-deploy osd create --data /dev/sdc --fs-type xfs ceph1
    ceph-deploy osd create --data /dev/sdd --fs-type xfs ceph1
    ceph-deploy osd create --data /dev/sdb --fs-type xfs ceph2
    ceph-deploy osd create --data /dev/sdc --fs-type xfs ceph2
    ceph-deploy osd create --data /dev/sdd --fs-type xfs ceph2

    ceph-deploy mds create ceph1 # ceph file-system only
    ceph-deploy mon add ceph2 # HA only
    ceph-deploy mgr create ceph2 # HA only
    ceph-deploy rgw create ceph1 # ceph object gateway, port: 7480
    ssh ceph1 ceph mgr module enable dashboard # default port: 7000
    ssh ceph1 ceph mgr module enable balancer


    此配置可在集群部署完后更改
    ceph.conf
        cluster_network = 192.168.56.0/24
        public_network = 172.16.0.0/24

    ceph admin --overrite ceph1 ceph2 ceph3
    restart services

删除集群
    ceph-deploy purge ceph1 ceph2
    # vgremove /dev/ceph-xxx
    # dd if=/dev/urandom of=/dev/sdb bs=512 count=64 如果出现无法创建osd, 执行此命令
    ceph-deploy purgedata ceph1 ceph2
    ceph-deploy forgetkeys
    cd ceph-cluster; rm -rf ceph*


RBD
    rbd可以只读模式被多重挂载, 但读写模式只能单个挂载

    ceph-deploy --username root install --no-adjust-repos client1
    ceph-deploy admin client1
    (admin-node): ceph osd pool create sata-pool 128 
    (admin-node): rados lspools # ceph osd lspools
    (admin-node): rbd pool init ssd-pool
    (admin-node): rbd create ssd-pool/foo --image-feature layering --size 4096M
    (admin-node): rbd info ssd-pool/foo
    (admin-node): rbd list ssd-pool
    (admin-node): rbd remove ssd-pool/foo
    (client1): rbd map ssd-pool/foo # -->/dev/rbd0
    (client1): rbd unmap ssd-pool/foo 
    (client1): rbd showmapped
    (admin-node): rbd --pool  ssd-pool du

