运维手册
    https://lihaijing.gitbooks.io/ceph-handbook/content/
    
设置pg
    http://docs.ceph.com/docs/master/rados/operations/placement-groups/
    设置pg_num, pgp_num
    Less than 5 OSDs set pg_num to 128
    Between 5 and 10 OSDs set pg_num to 512
    Between 10 and 50 OSDs set pg_num to 1024

    pg与pgp的关系
    1、 PGP起到对PG进行归置的作用。
    2、 PGP的取值应该与PG相同，在PG的值增大的同时，也要增大PGP的值以保持二者的值相同。
    3、 当一个POOL的PG增大后，Ceph并不会开始进行rebalancing，只有在PGP的值增大后，PG才会开始迁移至其他的OSD上，并且开始rebalancing。

ceph的优点
    crush算法
    ceph 的block device支持cow clone, incremental snapshot

ceph的使用
    对象存储
    块存储
    文件系统(需要metadata-server)

部署ceph client, 可以使用ceph-deploy工具 install, admin

pg出现下列状态的可能原因为replica设置不当, osd分布不均匀
    undersized: The placement group has fewer copies than the configured pool replication level. 
    degraded: Ceph has not replicated some objects in the placement group the correct number of times yet.

    ceph osd pool set sata-pool min_size 1
    ceph osd pool set sata-pool size 2




restart ceph services
    http://docs.ceph.com/docs/master/rados/operations/operating/


如何正确删除一个image
    https://www.cnblogs.com/boshen-hzb/p/6756484.html
    $ rbd showmapped
    $ rbd unmap foo
    $ rbd status foo # list watchers
    id pool image snap  device
    $ service rbdmap stop
    $ rbd rm rbd/foo

    rados -p rbd listwatchers myrbd.rbd


pool相关
    ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
    ceph osd pool rename {current-pool-name} {new-pool-name}
    rados df 
    ceph osd pool mksnap {pool-name} {snap-name} # 创建快照
    ceph osd pool rmsnap {pool-name} {snap-name} # 删除快照
    ceph osd pool get <pool-name> <config-key> # 获取配置
    ceph osd pool set {pool-name} {key} {value}

auth相关
    ceph auth list
    ceph auth get client.admin

出现rbd map挂载不上, 需要升级client内核
