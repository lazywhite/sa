 CMAN is a symmetric, general-purpose, kernel-based cluster manager. It has two parts. Connection Manager (cnxman) handles membership, messaging, quorum, event notification and transitions. Service Manager (sm) handles "service groups" which are a general way of representing and managing instances of external systems that require cluster management. The CMAN cluster manager is the foundational system upon which DLM, GFS, CLVM, and Fence all depend. The CMAN API in the kernel and userspace is general and available for other programs to use.

CMAN consists of a set of kernel patches and a userspace program (cman_tool). CMAN depends on CCS (only cman_tool which can optionally be used without CCS if all parameters are given on the command line.) 

------------------------------------------------
The Corosync Cluster Engine is a Group Communication System with additional features for implementing high availability within applications. The project provides four C Application Programming Interface features
从openais分裂出来的
------------------------------------------------

SAF（Service Availability Forum）
AIS（Application Interface specification）
AIS有两种开源实现，一种是openSAF，另外一种是openAIS


-------------------------------------------
SaltStack takes a new approach to infrastructure management by developing software that is easy enough to get running in minutes, scalable enough to manage tens of thousands of servers, and fast enough to communicate with them in seconds.

----------------------------------------------------

Cacti is a complete network graphing solution designed to harness the power of RRDTool's data storage and graphing functionality.

－－－－－－－－－－－－－－－－
Icinga is an enterprise grade open source monitoring system which keeps watch over networks and any conceivable network resource, notifies the user of errors and recoveries and generates performance data for reporting. Scalable and extensible, Icinga can monitor complex, large environments across dispersed locations.（nagois的升级版）
--------------------------------------------------------

pcs / pcs-gui (Pacemaker/Corosync Configuration System)

---------------------------------------------------
Keepalived is a routing software written in C. The main goal of this project is to provide simple and robust facilities for loadbalancing and high-availability to Linux system and Linux based infrastructures. Loadbalancing framework relies on well-known and widely used Linux Virtual Server (IPVS) kernel module providing Layer4 loadbalancing. Keepalived implements a set of checkers to dynamically and adaptively maintain and manage loadbalanced server pool according their health. On the other hand high-availability is achieved by VRRP protocol. VRRP is a fundamental brick for router failover. In addition, Keepalived implements a set of hooks to the VRRP finite state machine providing low-level and high-speed protocol interactions. Keepalived frameworks can be used independently or all together to provide resilient infrastructures.

----------------------------------------------------------

pacemaker是heartbeat v3里的CRM

---------------------------------------
Scale Up: 向上扩展
Scale Out: 向外扩展

负载均衡器：
	4层：性能高
	7层：支持特定的应用协议


监控：
	cacti, nagios(opsview,icinga)
	zabbix, zennos, opennms, ganglia

系统部署：pxe, cobbler
自动化运维：puppet, saltstack
批量运行命令：func, fabric, ansible

IaaS: 
PaaS:
SaaS


负载均衡集群：Load Balancing Cluster，LB
高可用集群：High Availiablity， HA
高性能集群：High Perfomance: HP
	

LB：
	传输层：内核空间
	应用层： 用户空间nginx, haproxy, apache, lighttpd, varnish, squid

	硬件：BigIP(F5), Netscaler(Citrix), A10

	Linux: lvs

HA: 99.9%，99%，99.999%
	heartbeat
	corosync(openais)
	RHCS(Red Hat Cluster Suite): cman
	keepalived
	ultramokey

HPC：
	
MapReduce: Hadoop(apache)
	Cloudera
	Hortonworkers
	EMC, Intel


HA: 至少3个；应该是奇数个节点
	paxos

	active/passive
	active/active
	N节点：m<n
		M-N 
		n-(n-1)

共享存储：MySQL
	NAS: network attached storage(文件级别)
	SAN:  storage area network(块级别)
	DAS：directly attached storage


集群文件系统：GFS2, OCFS2
		

分布式文件系统：HDFS、MooseFS、FastDFS、Swift、Lustre、TFS
		:只能增不能修改，现可支持追加




LVS: Linux Virtual Server
	ipvsadm/ipvs



调度器：Director, Dispatcher, Load Balancer


	
LVS类型：
	lvs-type：
		NAT: 
		DR
		TUN
		FullNAT


		NAT: 
			1、RealServer应该使用私有IP地址；
			2、RealServer的网关应该指向DIP；
			3、RIP和DIP应该在同一个网段内；
			4、进出的报文都得经过Directory，在高负载下，Directory会成为系统性能瓶颈；
			5、支持端口映射；
			6、RealServer可以使用任意OS；

		DR: Direct Routing
			1、RealServer可以使用私有地址；
			2、RealServer的网关一定不能指向DIP；
			3、RealServer和Director要在同一物理网络内；DIP和RIP应该在同一网段；
			4、入站报文经过Directory，出站则由RealServer直接响应Client；
			5、不能做端口映射；
			6、RealServer可以为大多数常见OS；

		TUN：Tunneling
			1、RIP、DIP不能是私有地址；
			2、RealServer的网关不能指向DIP；
			3、入站报文经过Directory，出站则由RealServer直接响应Client；
			4、不支持端口映射；
			5、支持IP tunneling的OS才能用于RealServer；
		FNAT：Full NAT
			1.DNAT＋SNAT
			2.RS可以不在一个VLAN
			3.响应报文必须要转回Director
			4.

LVS调度方法：
	静态(fixed method)：
		rr, wrr, sh, dh
			rr
			wrr
			sh: Source Hashing
			dh: Destination Hashing
	动态(Dynamic method)：
		lc, wlc, sed, nq, lblc, lblcr
			lc: Least Connection
				Overhead=Active*256+Inactive
			wlc: Weighted 
				Overhead=(Active*256+Inactive)/Weight
			sed: Shortest Expect Delay
				Overhead=(Active+1)*256/Weight
			nq: Never Queue
			lblc: (dh+lc) Locality-based Least Connection
			lblcr: Replicated and  Locality-based Least Connection



ipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]] [-O] [-M netmask]
	-A：添加
	-E：编辑

	-t: tcp
	-u: udp
	-f: firewall mark

	service-address:  VIP:Port
	-s: 指定调度方法

	ipvsadm -A -t 172.16.100.3:80 -s rr

ipvsadm -D -t|u|f service-address
	-D: 删除

ipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [-w weight] [-x upper] [-y lower]
	-a
	-e
	-r IP[:port]: 指定realserver地址
	指定lvs类型：
		-g: dr模型，默认
		-i: tun模型
		-m: nat模型
	-w weight

	ipvsadm -a -t 172.16.100.3:80 -r 192.168.10.7 -m 
	ipvsadm -a -t 172.16.100.3:80 -r 192.168.10.8 -m

ipvsadm -d -t|u|f service-address -r server-address
	-d: 删除realserver

	ipvsadm -d -t 172.16.100.3:80 -r 192.168.10.7 

保存：
service ipvsadm save
	/etc/sysconfig/ipvsadm

ipvsadm -S > /paht/to/ipvsadm.rules
ipvsadm -R < /path/to/ipvsadm.rules

ipvsadm -L|-l -n









禁止RS上的VIP直接跟前端路由通信的三种方案：
	1、修改路由，使用静态ARP；
	2、在RS上使用arptables，禁止响应对VIP的ARP广播请求；
	3、在RS上修改其内核参数,并向VIP配置在与RIP不同的接口的别名上；
		arp_ignore=1
		arp_announce=2


ipvsadm: 
管理集群服务
	-A, -E, -D
集群服务地址：
	-t IP:port
	-u IP:port
	-f #
	-s {rr|wrr|dh|sh|lc|wlc|sed|nq|lblc|lblcr}
		lc: Active*256+Inactive
		wlc: (Active*256+Inactive)/weight
		sed: (active+1)*256/weight
		nq: 


管理RS：
	-a, -e, -d, -r

保存规则：
	-S

恢复规则：
	-R

清空规则：
	-C

重置计数器：
	-Z

查看：-L|-l
	-n
	--rate
	--stats
	--timeout
	-c



sh: Source Hashing
	静态调度方法：wrr

lvs persistent template: hash table
	CIP: RS

	connection affinity
	port affinity

pcc: Persistent Client Connection
	持久客户端连接
ppc:Persistenc Port Connection
	持久端口连接




iptables:
	filter
	nat
	mangle
	raw

	iptables -t mangle -A PREROUTING -d $VIP -p tcp --dport 80 -j MARK --set-mark 10
	iptables -t mangle -A PREROUTING -d $VIP -p tcp --dport 443 -j MARK --set-mark 10

ipvsadm -A -f 10 -s rr -p
ipvsadm -a -f 10 -r IP -g 








DR类型中，Director和RealServer的配置脚本示例：

Director脚本:
#!/bin/bash
#
# LVS script for VS/DR
# chkconfig: - 90 10
#
. /etc/rc.d/init.d/functions
#
VIP=172.16.100.1
DIP=172.16.100.2
RIP1=172.16.100.7
RIP2=172.16.100.8
PORT=80
RSWEIGHT1=2
RSWEIGHT2=5

#
case "$1" in
start)           

  /sbin/ifconfig eth0:1 $VIP broadcast $VIP netmask 255.255.255.255 up
  /sbin/route add -host $VIP dev eth0:1

# Since this is the Director we must be able to forward packets
  echo 1 > /proc/sys/net/ipv4/ip_forward

# Clear all iptables rules.
  /sbin/iptables -F

# Reset iptables counters.
  /sbin/iptables -Z

# Clear all ipvsadm rules/services.
  /sbin/ipvsadm -C

# Add an IP virtual service for VIP 192.168.0.219 port 80
# In this recipe, we will use the round-robin scheduling method. 
# In production, however, you should use a weighted, dynamic scheduling method. 
  /sbin/ipvsadm -A -t $VIP:80 -s wlc

# Now direct packets for this VIP to
# the real server IP (RIP) inside the cluster
  /sbin/ipvsadm -a -t $VIP:80 -r $RIP1 -g -w $RSWEIGHT1
  /sbin/ipvsadm -a -t $VIP:80 -r $RIP2 -g -w $RSWEIGHT2

  /bin/touch /var/lock/subsys/ipvsadm &> /dev/null
;; 

stop)
# Stop forwarding packets
  echo 0 > /proc/sys/net/ipv4/ip_forward

# Reset ipvsadm
  /sbin/ipvsadm -C

# Bring down the VIP interface
  /sbin/ifconfig eth0:0 down
  /sbin/route del $VIP
  
  /bin/rm -f /var/lock/subsys/ipvsadm
  
  echo "ipvs is stopped..."
;;

status)
  if [ ! -e /var/lock/subsys/ipvsadm ]; then
    echo "ipvsadm is stopped ..."
  else
    echo "ipvs is running ..."
    ipvsadm -L -n
  fi
;;
*)
  echo "Usage: $0 {start|stop|status}"
;;
esac


RealServer脚本:

#!/bin/bash
#
# Script to start LVS DR real server.
# chkconfig: - 90 10
# description: LVS DR real server
#
.  /etc/rc.d/init.d/functions

VIP=172.16.100.1

host=`/bin/hostname`

case "$1" in
start)
       # Start LVS-DR real server on this machine.
        /sbin/ifconfig lo down
        /sbin/ifconfig lo up
        echo 1 > /proc/sys/net/ipv4/conf/lo/arp_ignore
        echo 2 > /proc/sys/net/ipv4/conf/lo/arp_announce
        echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
        echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce

        /sbin/ifconfig lo:0 $VIP broadcast $VIP netmask 255.255.255.255 up
        /sbin/route add -host $VIP dev lo:0

;;
stop)

        # Stop LVS-DR real server loopback device(s).
        /sbin/ifconfig lo:0 down
        echo 0 > /proc/sys/net/ipv4/conf/lo/arp_ignore
        echo 0 > /proc/sys/net/ipv4/conf/lo/arp_announce
        echo 0 > /proc/sys/net/ipv4/conf/all/arp_ignore
        echo 0 > /proc/sys/net/ipv4/conf/all/arp_announce

;;
status)

        # Status of LVS-DR real server.
        islothere=`/sbin/ifconfig lo:0 | grep $VIP`
        isrothere=`netstat -rn | grep "lo:0" | grep $VIP`
        if [ ! "$islothere" -o ! "isrothere" ];then
            # Either the route or the lo:0 device
            # not found.
            echo "LVS-DR real server Stopped."
        else
            echo "LVS-DR real server Running."
        fi
;;
*)
            # Invalid entry.
            echo "$0: Usage: $0 {start|status|stop}"
            exit 1
;;
esac





RS健康状态检查脚本示例第一版：
#!/bin/bash
#
VIP=192.168.10.3
CPORT=80
FAIL_BACK=127.0.0.1
FBSTATUS=0
RS=("192.168.10.7" "192.168.10.8")
RSTATUS=("1" "1")
RW=("2" "1")
RPORT=80
TYPE=g

add() {
  ipvsadm -a -t $VIP:$CPORT -r $1:$RPORT -$TYPE -w $2
  [ $? -eq 0 ] && return 0 || return 1
}

del() {
  ipvsadm -d -t $VIP:$CPORT -r $1:$RPORT
  [ $? -eq 0 ] && return 0 || return 1
}

while :; do
  let COUNT=0
  for I in ${RS[*]}; do
    if curl --connect-timeout 1 http://$I &> /dev/null; then
      if [ ${RSTATUS[$COUNT]} -eq 0 ]; then
         add $I ${RW[$COUNT]}
         [ $? -eq 0 ] && RSTATUS[$COUNT]=1
      fi
    else
      if [ ${RSTATUS[$COUNT]} -eq 1 ]; then
         del $I
         [ $? -eq 0 ] && RSTATUS[$COUNT]=0
      fi
    fi
    let COUNT++
  done
  sleep 5
done


RS健康状态检查脚本示例第二版：
#!/bin/bash
#
VIP=192.168.10.3
CPORT=80
FAIL_BACK=127.0.0.1
RS=("192.168.10.7" "192.168.10.8")
declare -a RSSTATUS
RW=("2" "1")
RPORT=80
TYPE=g
CHKLOOP=3
LOG=/var/log/ipvsmonitor.log

addrs() {
  ipvsadm -a -t $VIP:$CPORT -r $1:$RPORT -$TYPE -w $2
  [ $? -eq 0 ] && return 0 || return 1
}

delrs() {
  ipvsadm -d -t $VIP:$CPORT -r $1:$RPORT 
  [ $? -eq 0 ] && return 0 || return 1
}

checkrs() {
  local I=1
  while [ $I -le $CHKLOOP ]; do 
    if curl --connect-timeout 1 http://$1 &> /dev/null; then
      return 0
    fi
    let I++
  done
  return 1
}

initstatus() {
  local I
  local COUNT=0;
  for I in ${RS[*]}; do
    if ipvsadm -L -n | grep "$I:$RPORT" && > /dev/null ; then
      RSSTATUS[$COUNT]=1
    else 
      RSSTATUS[$COUNT]=0
    fi
  let COUNT++
  done
}

initstatus
while :; do
  let COUNT=0
  for I in ${RS[*]}; do
    if checkrs $I; then
      if [ ${RSSTATUS[$COUNT]} -eq 0 ]; then
         addrs $I ${RW[$COUNT]}
         [ $? -eq 0 ] && RSSTATUS[$COUNT]=1 && echo "`date +'%F %H:%M:%S'`, $I is back." >> $LOG
      fi
    else
      if [ ${RSSTATUS[$COUNT]} -eq 1 ]; then
         delrs $I
         [ $? -eq 0 ] && RSSTATUS[$COUNT]=0 && echo "`date +'%F %H:%M:%S'`, $I is gone." >> $LOG
      fi
    fi
    let COUNT++
  done 
  sleep 5
done


----------------------------------------------

curl命令选项：
	--cacert <file> CA证书 (SSL)
	--capath <directory> CA目录 (made using c_rehash) to verify peer against (SSL)
	--compressed 要求返回是压缩的形势 (using deflate or gzip)
	--connect-timeout <seconds> 设置最大请求时间
	-H/--header <line>自定义头信息传递给服务器
	-i/--include 输出时包括protocol头信息
	-I/--head 只显示文档信息
	--interface <interface> 使用指定网络接口/地址
	-s/--silent静音模式。不输出任何东西
	-u/--user <user[:password]>设置服务器的用户和密码
	-p/--proxytunnel 使用HTTP代理
------------------------




	资源黏性：资源更愿意留在哪个节点
	资源约束：
	       位置约束(资源和节点间的关系)：location
	       排列约束(资源和资源间的关系)：colocation
	       顺序约束(资源启动和关闭的次序)：order




Messaging Layer:
	heartbeat
	corosync
	cman

	keepalived(原理与前3者不同)

CRM:cluster messaging layer
	heartbeat v1-->haresources
	heartbeat v2-->crm
	heartbeat v3-->pacemaker

	RHCS: rgmanager

RA: resource agent
	heartbeat v1
	LSB
	OCF
	STONITH: Shoot The Other Node In The Head



编辑配置的接口
	1、直接编译配置文件；
	2、CLI：crmsh(SUSE), pcs(RedHat)
	3、GUI：pygui,hawk,pcs(web based),hb_gui




	


CRM：
	heartbeat v1: haresources
	heartbeat v2: crm
	heartbeat v3: pacemaker
	RHCS(rhel5): cman, rgmanager
	RHCS(rhel6): 
		heartbeat v3 + pacemaker
		cman + pacemaker
		corosync + pacemaker
		cman + rgmanager




非再可为集群节点时，处理策略
	stopped
	ignore
	freeze
	suicide

fencing:
	node:
		STONITH
	resouce:
		fence




资源类型：
	primitive, native: 主资源
	group: 组资源
	clone: 克隆资源
		stonith, cluster filesystem
	master/slave: 主、从类的资源
		drbd



集群模式：
	active/passive
	active/active
	n/m: n个节点，m服务
	n/n： n节点，n服务

双节点集群：
	1、ping node
		可以提供多个
	2、quoram disk
		仲裁磁盘



配置HA集群的前提：
	至少两个节点
	共享存储（可选）
	STONITH
	时间同步；
	主机名称解析
		/etc/hosts
		uname -n, hostname
	“已存在集群”
		至事先配置好一个节点：提供基本集群配置和配置文件，可以没有资源；
		ssh互信：

	集群级别要准备的工作：
		心跳信息：
			物理级别：串行线缆、以太网连接
			服务级别：单播、组播、广播
		ping node OR qdisk




安装heartbeat2

/etc/ha.d/
	authkeys: crc, md5, sha1，600
	ha.cf:
	haresources


＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

ningx --> Tengine
	平滑升级

I/O模型：阻塞，非阻塞
过程调用模型：异步、同步


I/O两段：
	同步阻塞
	同步非阻塞
	I/O复用  (同步)
	event-driven (同步)
	异步IO（异步）



nginx：
	web服务：
	rewrite：
	fastcgi：动静分离
	reverse proxy：反向代理
		redirect
	upstream：集群，节点健康状态检测
	cache：

组成：
	core：epoll，event
	http:
		standart
		optional
	smtp:
		pop3
		smtp
		proxy
	third-party
		auth-pam






worker_processes number | auto; 定义worker进程的个数; 
	clients = worker_connections * work_processes

worker_cpu_affinity cpumask ...;
	0001 0010 0100 1000


打开文件数目的限制：
	ulimit -n
	/etc/security/limits.conf
	nginx nofile 51200

worker_rlimit_nofile number;    51200

worker_priority number;
	-20,20

user
error_log
events {
	use 
	worker_connections 51200;
}
worker_processes 4;
worker_cup_affinity cpumask;
worker_rlimit_nofile 51200;
worker_priority -5;





= : 做精确匹配
~ : 模式匹配, 区分字符大小写
~*: 模式匹配，但不区分字符大小写
^~: 不做模式匹配

优先级：=, ^~, ~或~*, 




location /status {
	stub_status on;
	access_log off;	
}





rewrite regex replacement []
rewrite regex
rewrite

rewrite /downloads/(.*)$  /images/$1 break;
rewrite /images/(.*\.jpg)$  /downloads/$1 break;



如果请求的URI为.jpg, .png, .gif，

if ($request_uri ~* \.(jpg|png|gif)$) {
	
}

location ~* \.(jpg|png|gif)$ {
	
}


www.test.com, 

if ($host == "www.test.com") {
	
}

if ($remote_addr)

应用上下文：server, location




location / {
    proxy_pass       http://localhost:8000;
    proxy_set_header Host      $host;
    proxy_set_header X-Real-IP $remote_addr;
}


DAV: 


页面
反向代理缓存
日志缓存
fastcgi缓存

1、反向代理；并且后端服务在日志中记录真实客户端IP；
			 php: 有特定的服务器服务；
			 静态内容:特定的服务器
			 dav:特定的服务器；
2、upstream：以负载均衡的方式将用户请求代理至多台不同的服务器；






reverse proxy:
	nginx: 
	haproxy
cache server:
	squid
	varnish

proxy:


fastcgi协议取得的内容也可以缓存，缓存使用fastcgi_cache_path定义，使用时用fastcgi_cache启用；

open_log_file_cache max=1000 inactive=20s valid=1m min_uses=2;



nginx的负载均衡调度方法：
	rr, ip_hash, least_conn



nginx命令行参数：
	-t: 测试配置文件语法
	-s stop: kill -SIGTERM, kill -SIGKILL
	-s quit: kill -SIGQUIT <master_process_id>
	-s quit: kill -SIGWINCH <work_process_id>

平滑升级nginx：
	kill -SIGUSER2 <master_process_id>

日志轮换：
	-s reopen: kill -SIGUSER1 <master_process_id>

重读配置文件：
	-s reload: kill -SIGHUP <master_process_id>





rewrite:
	last
	break
	redirect
	permanent





资源管理器：
	heartbeat v1: haresources
	heartbeat v2: crm
	heartbeat v3: pacemaker

	corosync: pacemaker
	cman: rgmanager

	RHEL 6.4：

	RA: 
		class：类别
			heartbeat v1 legacy
			ocf:
				heartbeat
				pacemaker
				linbit
				redhat
			lsb
			stonith
		provider： 提供者

PE：Policy engine
DC

crm:
	资源约束：
		location：资源对节点的偏好
		colocation：资源运行在同一个节点上的可能性
		order： 资源采取动作的次序

	集群属性：
		全局属性
	资源属性：
		failover
		failback

	隔离：
		node：stonith
		resource：fencing


	法定票数：
		with quorum
		without quorum


心跳信息传递：
	ucast
	mcast
	bcast

rhel6: corosync+pacemaker
	结合方式有两种：
		1、pacemaker作为corosync的插件运行；
		2、pacemaker作为独立的守护进程运行；



heartbeat v1:
	messaging layer: /etc/ha.d/ha.cf
	haresources: /etc/ha.d/haresources
	crm: /var/lib/heartbeat/crm/cib.xml





资源类型：
	primitive
	group
	master/slave
	clone


集群框架：openais
集群构建
	基于heartbeat（v1,v2,v3)
	基于corosync  (pacemaker)
	RHCS套件：cman＋rgmanager(conga)



配置工具
	CLI
	GUI
	XML

CentOS 6.4:
corosync+pacemaker
heartbeat v3 + pacemaker
cman + rgmanager
cman + pacemaker

corosync: /etc/corosync/corosync.conf




crm（由pacemaker提供）
	ra
	resource
	configure
		primitive
		group
		ms
		clone

		order
		location
		colocation

		show
		show xml

		edit
		verify

		commit

		op
			start
			stop
			minitor: 0

op monitor interval=# timeout=# on-fail=restart
---------------------------------------

pcs:


pcs status = crm status
pcs config = crm configure show

crm ra classes = pcs resource standards
				 pcs resource providers

crm ra list CLASS = pcs resource agents CLASS



no-quorum-policy=
Allowed values: stop, freeze, ignore, suicide


MySQL:
	myip, mysqld, mystore(Filesystem)

	primitive myip ocf:heartbeat:IPaddr params ip=172.16.100.12 cidr_netmask=16 nic=eth0 op monitor interval=20 timeout=20 on-fail=restart
	primitive mystore ocf:heartbeat:Filesystem params device="172.16.100.17:/mydata" directory="/mydata" fstype="nfs" op monitor interval=20 timeout=20 on-fail=restart
	primitive myserver lsb:mysqld op monitor interval=20 timeout=20 on-fail=restart

	group myservice myip mystore myserver

	order myip_berfore_mystore_before_myserver mandatory: (myip mystore) myserver



	pcs resource create myip ocf:heartbeat:IPaddr params ip=172.16.100.12 cidr_netmask=16 nic=eth0 op monitor interval=20 timeout=20 on-fail=restart

	pcs resource create mystore ocf:heartbeat:Filesystem params device="172.16.100.17:/mydata" directory="/mydata" fstype="nfs" op monitor interval=20 timeout=20 on-fail=restart

	pcs resource create myserver lsb:mysqld op monitor interval=20 timeout=20 on-fail=restart

	pcs resource group add myservice myip mystore myserver

	pcs constraint order myip then mystore then myserver





rhel6, centos6
	drbd
	drbd-kmdl


corosync+packmaker

crmsh
pcs

drbd

stonith-enabled={true|false}

crm configure property stonith-enabled=false
pcs property set stonith-enabled=false

crm configure primitive mystore ocf:heartbeat:Filesystem params device="172.16.100.17:/mydata" directory="/mydata" fstype="nfs" op monitor interval=20 timeout=20 on-fail=restart

pcs resource create mystore ocf:heartbeat:Filesystem params device="172.16.100.17:/mydata" directory="/mydata" fstype="nfs" op monitor interval=20 timeout=20 on-fail=restart

drbd: Distributed Replicated Block Device
	磁盘镜像
	primary/secondary

	drbd

	resource:
		name、device、disk、net

	common {

	}

	global {

	}

	drbdadm, drbdset, drbdmeta

cat /proc/drbd
drbd-overview

drbdadm primary mydata

drbdadm secondary mydata

protocol:
	A
	B
	C

HA

双主模式：1、HA；2、集群文件系统



myip, myserver, mysql_drbd, ms_mysql_drbd, mystore

group

order 
	ms_mysql_drbd:Master  then mystore, myip then myserver

order master_before_mystore_and_myip_before_myserver mandatory: ms_mysql_drbd:Master (mystore:start myip:start) myserver:start


pacemaker官方文档：
	http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html-single/Pacemaker_Explained/index.html



RHCS: Red Hat Cluster Suite(套件)
	RHEL6: cman+rgmanager
	RHEL7: cman+pacemaker (pacemaker将成为独立的守护进程)


RHEL 6.4: keepalived, haproxy

	heartbeat v3 + pacemaker
	corosync + pacemaker
	cman + pacemaker
	cman + rgmanager


GFS, VFS

RHCS: gfs2, cLVM

RHCS: 三个节点, qdisk

cman, rgmanager

cman: /etc/cluster/cluster.conf
	xml

	Conga：
		C/S
		master/agent


expect: 


查询日志
慢查询日志
错误日志
二进制日志
事务日志(innodb)
中继日志


二进制日志：
	position
	server id
	时间
	事件

SHOW MASTER STATUS
SHOW BINLOG EVENTS

mysqlbinlog

	基于语句
	基于行
	混合

	statement
	row
	mixed

数据分布
负载均衡
备份
高可用性和故障切换
MySQL升级测试


主从复制：
	主服务器：
		创建具有复制权限的用户帐号；
		设置server-id；
		启用二进制日志；

	从服务器：
		启用中继日志；（可选：关闭二进制日志）
		设置server-id；
		启动复制线程; 

	基于ssl的复制；
	mysql-5.5支持半同步复制；

MySQL-5.5.33

node1.test.com: 172.16.100.15
node2.test.com: 172.16.100.16

mysql版本要一致：
	主低，从高；

完全备份：
	mysqldump
	lvm
	xtraback


mysql> SHOW MASTER STATUS
SHOW SLAVE STATUS\G
SHOW PROCESSLIST;

START SLAVE {IO_THREAD|SQL_THREAD};

CHANGE MASTER TO MASTER_HOST='',MASTER_USER='',MASTER_PASSWORD='',MASTER_LOG_FILE='',MASTER_LOG_POS=

为了复制安全：
	从服务器：
		read_only=ON
		skip-slave-start=ON

	主服务器：
		sync_binlog=ON



cman, drbd, mysql复制：

	cman: Messaging Layer
		/etc/cluster/cluster.conf

		cman+rgmanager:
			resource group
			service

		Conga:
			luci+ricci

		failover domain:

	cman+pacemaker:
		service cman start
		service pacemaker start

drbd: primary/secondary
	master/slave

HA: 
	平均无故障时间/(平均无故障时间+平均修复时间)
	95%, 99%, 99.5%, 99.9%, 99.99%, 99.999%

	nginx, haproxy, ipvs

mysql复制：
	平均负载
		主：读写
		从：读
	数据冗余：
	升级测试
	数据分布

	二进制日志

	中继日志

DAS, NAS, SAN

IP SAN

cman 


	adapter
	controller

		IDE: 并行, 133MB/s
			/dev/hd
		SCSI：Small Computer System Interface
			并行, Ultra320, 320MB/s

			窄总线：8（7）
			宽总线：16（15）

		Gb GB

		IDE：ATA --> SATA(6Gbps=768MB/s)
		SCSI: SAS


		RAID：

		SSD: 
			SATA
			PCI-E
				Fusion IO
				华为

		USB: 480MB/S

DAS
SAN: 块级别共享存储
	FC SAN
	IP SAN
	FCoE
NAS: 文件级别共享存储
	nfs, smb, cifs		


iSCSI认证方式：
	基于IP的认证；
	基于用户的认证；
		CHAP: 

如何持久共享的iSCSI Target和LUN：
	/etc/rc.d/rc.local
	/etc/tgt/target.conf

	tgtd服务：

initiator: 
	iscsi-initiator-utils 
		iscsi数据库

	服务：iscsi, iscsid

NAS: CIFS, NFS, SMB
SAN: iSCSI


iSNS: iSCSI

tgtadm: 模式化命令，三种模式
	target
	logicalunit
	account

模式中的操作：
	target:
		show
		new
		delete
		update
		bind
		unbind

	logicalunit:
		new
		delete

	target的命令机制：iqn
		iqn.yyyy-mm.reverse_domain.STRING[:substring]

		iqn.2013-09.com.test.tgt1:disk1

		[-L --lld <driver>] [-o --op <operation>] [-m --mode <mode>] [-t --tid <id>]
                           [-T --targetname <targetname>]

        [-l --lun <lun>] [-b --backing-store <path>]
                           [-E --bstype <type>] [-I --initiator-address <address>]


initiator:
	iscsiadm：模式化命令
		discovery
		node

	类型：
		-t sendtargets
		-t st


总结：
	发现：iscsiadm -m discovery -t sendtarets -p IP:PORT 
	登录：iscsiadm -m node -T TARGET_NAME -p IP:PORT -l


iSCSI: 
	tcp/3260

	SCSI: SAS
	IDE: SATA
	USE
	RAID

DAS
SAN: 块级别
	FC SAN
	IP SAN
NAS: 文件级别

CentOS 6
	iSCSI server: target
		scsi-target-utils
	iSCSI client: initiator
		iscsi-initiator-utils

scsi-target-utils
	tgtadm
		target
		logicalunit
		account

	target:
		new、delete、show、update、bind、unbind

	logicalunit:
		new、delete


iscsi-initiator-utils
	iscsiadm
		discovery
		node


tgtadm:
	-L iscsi
	-t
	-m
	-o
	-T
	-l
	-I 
	-b


用户认证：CHAP

<IncomingUser> 
<OutgoingUser>


GFS: Global File System

        当多个节点需要操作同一个文件系统时
        GFS2, OCFS2

DFS: Distributed File System
        HDFS
        TFS
        MooseFS
        MogileFS
        FastDFS
        lustre
        ceph

GFS2:
        1、共享式文件系统；
        2、日志
        3、64bit
        4、兼容POSIX
        5、读和回写缓存
        6、支持文件系统在线扩展，支持动态inode数目；
        7、支持直接I/O
        8、配额
        9、ACL
        10、SELinux

32bits: 16TB
64bits: 8EB

cman+rgmanager: gfs2
corosync+pacemaker: gfs2
        依赖于gfs2_controlled, dlm_controlled

cLVM: Cluster LVM





DAS, NAS, SAN

NAS
SAN: SCSI
        FC: Fiber Channel
        TCP/IP(IP SAN): iSCSI
        FCoE
        IB 

iSCSI: 
        HBA
        ToE
        Software

iSCSI: 


CFS: GFS2
        mkfs -t

        cLVM: lvm2-cluster

DFS


HA: 
        heartbeat
        corosync(openais)
        cman

        keepalived: ipvs()


keepalived:
        vrrp:
        virtual server
        vrrp_script

/etc/keepalived/keepalived.conf

/etc/rc.d/init.d/keepalived

CentOS 6.4: 1.2.7



邮件服务器：
        rhel5: sendmail
        rhel6: postfix


vrrp_script {
        
}


vrrp_instance {
        
        track_script {

        }
        
}



1、如何在状态转换时进行通知？
        notify_master ""
        notify_backup
        notify_fault 

        vrrp_sync_group {

        }

        vrrp_instance {

        }

MASTER:
#!/bin/bash
#
vip=172.16.100.100
contact='root@localhost'
thisip=`ifconfig eth0 | awk '/inet addr:/{print $2}' | awk -F: '{print $2}'`

notify() {
        mailbody="vrrp transaction, $vip floated to $thisip."
        subject="$thisip is to be $vip master"
        echo $mailbody | mail -s $subject $contact
}

notify



2、如何配置ipvs？
        virutal server
        realserver
                health check

        tcp, udp, fwm


        virtual_server VIP PORT {
                lb_kind
                lb_algo
                real_server RIP PORT {
                        weight #

                }

                real_server RIP PORT {
                        weight #

                } 

        }



virtual_server 172.16.100.100 80 {
    delay_loop 6
    lb_algo rr
    lb_kind DR
    nat_mask 255.255.0.0
    persistence_timeout 0
    protocol TCP

    real_server 172.16.100.17 80 {
        weight 1
        HTTP_GET {
            url {
              path /
              status_code 200
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
    real_server 172.16.100.18 80 {
        weight 1
        HTTP_GET {
            url {
              path /
              status_code 200
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}


3、如何对某特定服务做高可用？

1、监控服务
vrrp_script {
        
}

2、在vrrp实例中追踪服务
track_script {
        
}

nginx



4、如何实现基于多虚拟路由的master/master模型？



web: 
        haproxy, tomcat, apache, nginx, varnish








-------------------------------------
drbd: distributed replicatied block device

dlm: Distributed Lock Manager

Cluster Filesystem:
	GFS2: Global File System
	OCFS2: Oracle Cluster File System


------------------------------------
conga:luci+ricci+cman+rgmanager
conga以服务的形式控制集群(可控制多个集群），实际的crm是rgmanager，底层是cman



Q. What is Conga?
A. Conga is an agent/server architecture for remote administration of systems.
The agent component is called "ricci", and the server is called "luci".
One luci server can communicate with many multiple ricci agents installed on systems.
The luci server is accessed via a browser using https.

Q. What can Conga do?
A. Conga has been initially developed to provide a convenient
method for creating and managing clusters built with Red Hat Cluster Suite.
It also offers an interface for managing sophisticated storage configurations
like those often built to support clusters.

One of the richest interface tools Conga provides is for managing Xen VMs as clustered services. VMs can be created, migrated, and can
take advantage of the benefits that traditional clustered services have, such as high availability, failover domain configuration/membership, heartbeat monitoring, and
autostart. 

主节点安装luci，注意屏蔽epel源，然后启动luci服务
从节点安装ricci，开启ricci服务
设定ricci用户的密码
访问https://master:8084/
添加cluster，添加节点,添加资源


